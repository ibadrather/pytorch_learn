{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e20dc0",
   "metadata": {},
   "source": [
    "###### Source: \n",
    "https://towardsdatascience.com/pytorch-lstms-for-time-series-data-cd16190929d7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76521152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41087501",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # number of samples\n",
    "L = 1000 # length of each sample (number of values for each sine wave)\n",
    "T = 20 # width of the wave\n",
    "x = np.empty((N,L), np.float32) # instantiate empty array\n",
    "x[:] = np.arange(L) + np.random.randint(-4*T, 4*T, N).reshape(N,1)\n",
    "y = np.sin(x/1.0/T).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5240b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_layers=64):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        # lstm1, lstm2, linear are all layers in the network\n",
    "        self.lstm1 = nn.LSTMCell(1, self.hidden_layers)\n",
    "        self.lstm2 = nn.LSTMCell(self.hidden_layers, self.hidden_layers)\n",
    "        self.linear = nn.Linear(self.hidden_layers, 1)\n",
    "        \n",
    "    def forward(self, y, future_preds=0):\n",
    "        outputs, n_samples = [], y.shape[0]\n",
    "        h_t = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        c_t = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        h_t2 = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        c_t2 = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        \n",
    "        for time_step in y.split(1, dim=1):\n",
    "            # N, 1\n",
    "            h_t, c_t = self.lstm1(time_step, (h_t, c_t)) # initial hidden and cell states\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2)) # new hidden and cell states\n",
    "            output = self.linear(h_t2) # output from the last FC layer\n",
    "            outputs.append(output)\n",
    "            \n",
    "        for i in range(future_preds):\n",
    "            # this only generates future predictions if we pass in future_preds>0\n",
    "            # mirrors the code above, using last output/prediction as input\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs.append(output)\n",
    "        # transform list to tensor    \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bb50a6",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd8271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1000)\n",
      "torch.Size([97, 999])\n",
      "torch.Size([97, 999])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "train_input = torch.from_numpy(y[3: ,:-1])\n",
    "train_target = torch.from_numpy(y[3:, 1:])\n",
    "print(train_input.shape)\n",
    "print(train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b6ce825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 999])\n",
      "torch.Size([3, 999])\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.from_numpy(y[:3 ,:-1])\n",
    "test_target = torch.from_numpy(y[:3, 1:])\n",
    "print(test_input.shape)\n",
    "print(test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0808049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = torch.optim.LBFGS(model.parameters(), lr=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac1a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, model, optimiser, loss_fn, \n",
    "                  train_input, train_target, test_input, test_target):\n",
    "    for i in tqdm(range(n_epochs)):\n",
    "        def closure():\n",
    "            model.train()\n",
    "            optimiser.zero_grad()\n",
    "            out = model(train_input)\n",
    "            loss = loss_fn(out, train_target)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimiser.step(closure)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            future = 1000\n",
    "            pred = model(test_input, future=future)\n",
    "            # use all pred samples, but only go to 999\n",
    "            loss = loss_fn(pred[:, :-future], test_target)\n",
    "            y = pred.detach().numpy()\n",
    "        # draw figures\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.title(f\"Step {i+1}\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        n = train_input.shape[1] # 999\n",
    "        def draw(yi, colour):\n",
    "            plt.plot(np.arange(n), yi[:n], colour, linewidth=2.0)\n",
    "            plt.plot(np.arange(n, n+future), yi[n:], colour+\":\", linewidth=2.0)\n",
    "        draw(y[0], 'r')\n",
    "        draw(y[1], 'b')\n",
    "        draw(y[2], 'g')\n",
    "        plt.savefig(\"predict%d.png\"%i, dpi=300)\n",
    "        plt.close()\n",
    "        # print the loss\n",
    "        out = model(train_input)\n",
    "        loss_print = loss_fn(out, train_target)\n",
    "        print(\"Step: {}, Loss: {}\".format(i, loss_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43420d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/10 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'future'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m              \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtrain_target\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtest_target\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_target\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(n_epochs, model, optimiser, loss_fn, train_input, train_target, test_input, test_target)\u001b[0m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     14\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m---> 15\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# use all pred samples, but only go to 999\u001b[39;00m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred[:, :\u001b[38;5;241m-\u001b[39mfuture], test_target)\n",
      "File \u001b[0;32m~/anaconda3/envs/ronin/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'future'"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs = 10,\n",
    "              model = model,\n",
    "              optimiser = optimiser,\n",
    "              loss_fn = criterion,\n",
    "              train_input = train_input,\n",
    "              train_target = train_target,\n",
    "              test_input = test_input,\n",
    "              test_target = test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee242135",
   "metadata": {},
   "source": [
    "### Automating model construction\n",
    "\n",
    "This whole exercise is pointless if we still can’t apply an LSTM to other shapes of input. Let’s generate some new data, except this time, we’ll randomly generate the number of curves and the samples in each curve. We won’t know what the actual values of these parameters are, and so this is a perfect way to see if we can construct an LSTM based on the relationships between input and output shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10317297",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.random.randint(50, 200) # number of samples\n",
    "L = np.random.randint(800, 1200) # length of each sample (number of values for each sine wave)\n",
    "T = np.random.randint(10, 30) # width of the wave\n",
    "x = np.empty((N,L), np.float32) # instantiate empty array\n",
    "x[:] = np.arange(L) + np.random.randint(-4*T, 4*T, N).reshape(N,1)\n",
    "y = np.cos(np.sin(x/1.0/T)**2).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7aeb1a",
   "metadata": {},
   "source": [
    "We could then change the following input and output shapes by determining the percentage of samples in each curve we’d like to use for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prop = 0.95\n",
    "train_samples = round(N * train_prop) \n",
    "test_samples = N - train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input and output shapes thus become:\n",
    "\n",
    "# y = (N, L)\n",
    "train_input = torch.from_numpy(y[test_samples:, :-1]) # (train_samples, L-1)\n",
    "train_target = torch.from_numpy(y[test_samples:, 1:]) # (train_samples, L-1)\n",
    "test_input = torch.from_numpy(y[:test_samples, :-1]) # (train_samples, L-1)\n",
    "test_target = torch.from_numpy(y[:test_samples, 1:]) # (train_samples, L-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558b3f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Input\", train_input.shape)\n",
    "print(\"Train Target\", train_target.shape)\n",
    "print(\"Test Input\", test_input.shape)\n",
    "print(\"Test Target\", test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880be05c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
