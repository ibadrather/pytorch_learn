{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Fine-Tuning\n",
    "\n",
    "Multi-head fine-tuning refers to the practice of fine-tuning a pre-trained model for multiple tasks simultaneously. This is often done by adding multiple \"heads\" to a shared \"base\" model. Each head is responsible for a specific task. The idea is that the shared layers learn general features that are useful for all tasks, while each head specializes in its own task.\n",
    "\n",
    "For example, in a natural language processing scenario, you might have one head for sentiment analysis and another for named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 1;\n                var nbb_unformatted_code = \"# !sudo apt-get install libopenmpi-dev\\n# !sudo apt install nvidia-cuda-toolkit\";\n                var nbb_formatted_code = \"# !sudo apt-get install libopenmpi-dev\\n# !sudo apt install nvidia-cuda-toolkit\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !sudo apt-get install libopenmpi-dev\n",
    "# !sudo apt install nvidia-cuda-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"%pip install torch==2.0.1 transformers deepspeed mpi4py --quiet\";\n                var nbb_formatted_code = \"%pip install torch==2.0.1 transformers deepspeed mpi4py --quiet\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install torch==2.0.1 transformers deepspeed mpi4py --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-23 15:53:33,783] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 3;\n                var nbb_unformatted_code = \"import torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\\n\\nfrom transformers import BertModel, BertTokenizer\\n\\nimport deepspeed\";\n                var nbb_formatted_code = \"import torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\\n\\nfrom transformers import BertModel, BertTokenizer\\n\\nimport deepspeed\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "import deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 4;\n                var nbb_unformatted_code = \"torch.__version__\";\n                var nbb_formatted_code = \"torch.__version__\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 5;\n                var nbb_unformatted_code = \"class MultiHeadModel(nn.Module):\\n    def __init__(self):\\n        super(MultiHeadModel, self).__init__()\\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\\n        \\n        # Sentiment analysis head (binary classification)\\n        self.sentiment_head = nn.Linear(768, 1)\\n        \\n        # Named entity recognition head (let's assume 10 classes)\\n        self.ner_head = nn.Linear(768, 10)\\n        \\n    def forward(self, input_ids, attention_mask):\\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\\n        last_hidden_state = outputs.last_hidden_state\\n        \\n        # For sentiment analysis, we'll just use the [CLS] token representation\\n        cls_token = last_hidden_state[:, 0, :]\\n        sentiment_output = self.sentiment_head(cls_token)\\n        \\n        # For NER, we'll use the representation for each token\\n        ner_output = self.ner_head(last_hidden_state)\\n        \\n        return sentiment_output, ner_output\";\n                var nbb_formatted_code = \"class MultiHeadModel(nn.Module):\\n    def __init__(self):\\n        super(MultiHeadModel, self).__init__()\\n        self.bert = BertModel.from_pretrained(\\\"bert-base-uncased\\\")\\n\\n        # Sentiment analysis head (binary classification)\\n        self.sentiment_head = nn.Linear(768, 1)\\n\\n        # Named entity recognition head (let's assume 10 classes)\\n        self.ner_head = nn.Linear(768, 10)\\n\\n    def forward(self, input_ids, attention_mask):\\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\\n        last_hidden_state = outputs.last_hidden_state\\n\\n        # For sentiment analysis, we'll just use the [CLS] token representation\\n        cls_token = last_hidden_state[:, 0, :]\\n        sentiment_output = self.sentiment_head(cls_token)\\n\\n        # For NER, we'll use the representation for each token\\n        ner_output = self.ner_head(last_hidden_state)\\n\\n        return sentiment_output, ner_output\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MultiHeadModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Sentiment analysis head (binary classification)\n",
    "        self.sentiment_head = nn.Linear(768, 1)\n",
    "        \n",
    "        # Named entity recognition head (let's assume 10 classes)\n",
    "        self.ner_head = nn.Linear(768, 10)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        \n",
    "        # For sentiment analysis, we'll just use the [CLS] token representation\n",
    "        cls_token = last_hidden_state[:, 0, :]\n",
    "        sentiment_output = self.sentiment_head(cls_token)\n",
    "        \n",
    "        # For NER, we'll use the representation for each token\n",
    "        ner_output = self.ner_head(last_hidden_state)\n",
    "        \n",
    "        return sentiment_output, ner_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-23 15:53:37,759] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown\n",
      "[2023-09-23 15:53:37,759] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-09-23 15:53:37,760] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2023-09-23 15:53:39,781] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=192.168.0.113, master_port=29500\n",
      "[2023-09-23 15:53:39,782] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2023-09-23 15:53:41,149] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-09-23 15:53:41,151] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2023-09-23 15:53:41,152] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2023-09-23 15:53:41,162] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2023-09-23 15:53:41,163] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 unfused optimizer with dynamic loss scale\n",
      "[2023-09-23 15:53:41,163] [INFO] [unfused_optimizer.py:45:__init__] Fused Lamb Legacy : False \n",
      "[2023-09-23 15:53:41,212] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2023-09-23 15:53:41,213] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2023-09-23 15:53:41,213] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2023-09-23 15:53:41,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[(0.9, 0.999)]\n",
      "[2023-09-23 15:53:41,215] [INFO] [config.py:967:print] DeepSpeedEngine configuration:\n",
      "[2023-09-23 15:53:41,216] [INFO] [config.py:971:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-09-23 15:53:41,216] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-09-23 15:53:41,217] [INFO] [config.py:971:print]   amp_enabled .................. False\n",
      "[2023-09-23 15:53:41,217] [INFO] [config.py:971:print]   amp_params ................... False\n",
      "[2023-09-23 15:53:41,218] [INFO] [config.py:971:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-09-23 15:53:41,219] [INFO] [config.py:971:print]   bfloat16_enabled ............. False\n",
      "[2023-09-23 15:53:41,219] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-09-23 15:53:41,220] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-09-23 15:53:41,221] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-09-23 15:53:41,222] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f19603f61d0>\n",
      "[2023-09-23 15:53:41,223] [INFO] [config.py:971:print]   communication_data_type ...... None\n",
      "[2023-09-23 15:53:41,223] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-09-23 15:53:41,224] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False\n",
      "[2023-09-23 15:53:41,224] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False\n",
      "[2023-09-23 15:53:41,225] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-09-23 15:53:41,225] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False\n",
      "[2023-09-23 15:53:41,226] [INFO] [config.py:971:print]   dataloader_drop_last ......... False\n",
      "[2023-09-23 15:53:41,226] [INFO] [config.py:971:print]   disable_allgather ............ False\n",
      "[2023-09-23 15:53:41,227] [INFO] [config.py:971:print]   dump_state ................... False\n",
      "[2023-09-23 15:53:41,227] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... None\n",
      "[2023-09-23 15:53:41,228] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False\n",
      "[2023-09-23 15:53:41,228] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-09-23 15:53:41,229] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-09-23 15:53:41,229] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-09-23 15:53:41,230] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-09-23 15:53:41,230] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-09-23 15:53:41,230] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-09-23 15:53:41,231] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False\n",
      "[2023-09-23 15:53:41,232] [INFO] [config.py:971:print]   elasticity_enabled ........... False\n",
      "[2023-09-23 15:53:41,233] [INFO] [config.py:971:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-09-23 15:53:41,234] [INFO] [config.py:971:print]   fp16_auto_cast ............... False\n",
      "[2023-09-23 15:53:41,234] [INFO] [config.py:971:print]   fp16_enabled ................. True\n",
      "[2023-09-23 15:53:41,235] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-09-23 15:53:41,235] [INFO] [config.py:971:print]   global_rank .................. 0\n",
      "[2023-09-23 15:53:41,236] [INFO] [config.py:971:print]   grad_accum_dtype ............. None\n",
      "[2023-09-23 15:53:41,236] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1\n",
      "[2023-09-23 15:53:41,237] [INFO] [config.py:971:print]   gradient_clipping ............ 0.0\n",
      "[2023-09-23 15:53:41,237] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-09-23 15:53:41,238] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-09-23 15:53:41,238] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-09-23 15:53:41,238] [INFO] [config.py:971:print]   load_universal_checkpoint .... False\n",
      "[2023-09-23 15:53:41,239] [INFO] [config.py:971:print]   loss_scale ................... 0\n",
      "[2023-09-23 15:53:41,244] [INFO] [config.py:971:print]   memory_breakdown ............. False\n",
      "[2023-09-23 15:53:41,245] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False\n",
      "[2023-09-23 15:53:41,245] [INFO] [config.py:971:print]   mics_shard_size .............. -1\n",
      "[2023-09-23 15:53:41,246] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-09-23 15:53:41,246] [INFO] [config.py:971:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-09-23 15:53:41,247] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-09-23 15:53:41,247] [INFO] [config.py:971:print]   optimizer_name ............... adam\n",
      "[2023-09-23 15:53:41,248] [INFO] [config.py:971:print]   optimizer_params ............. {'lr': 0.1}\n",
      "[2023-09-23 15:53:41,248] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-09-23 15:53:41,249] [INFO] [config.py:971:print]   pld_enabled .................. False\n",
      "[2023-09-23 15:53:41,249] [INFO] [config.py:971:print]   pld_params ................... False\n",
      "[2023-09-23 15:53:41,250] [INFO] [config.py:971:print]   prescale_gradients ........... False\n",
      "[2023-09-23 15:53:41,250] [INFO] [config.py:971:print]   scheduler_name ............... None\n",
      "[2023-09-23 15:53:41,250] [INFO] [config.py:971:print]   scheduler_params ............. None\n",
      "[2023-09-23 15:53:41,251] [INFO] [config.py:971:print]   sparse_attention ............. None\n",
      "[2023-09-23 15:53:41,251] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False\n",
      "[2023-09-23 15:53:41,251] [INFO] [config.py:971:print]   steps_per_print .............. 10\n",
      "[2023-09-23 15:53:41,252] [INFO] [config.py:971:print]   train_batch_size ............. 8\n",
      "[2023-09-23 15:53:41,252] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  8\n",
      "[2023-09-23 15:53:41,257] [INFO] [config.py:971:print]   use_node_local_storage ....... False\n",
      "[2023-09-23 15:53:41,258] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False\n",
      "[2023-09-23 15:53:41,259] [INFO] [config.py:971:print]   weight_quantization_config ... None\n",
      "[2023-09-23 15:53:41,260] [INFO] [config.py:971:print]   world_size ................... 1\n",
      "[2023-09-23 15:53:41,260] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False\n",
      "[2023-09-23 15:53:41,261] [INFO] [config.py:971:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2023-09-23 15:53:41,262] [INFO] [config.py:971:print]   zero_enabled ................. False\n",
      "[2023-09-23 15:53:41,264] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-09-23 15:53:41,265] [INFO] [config.py:971:print]   zero_optimization_stage ...... 0\n",
      "[2023-09-23 15:53:41,266] [INFO] [config.py:957:print_user_config]   json = {\n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.1\n",
      "        }\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"train_micro_batch_size_per_gpu\": 8\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 6;\n                var nbb_unformatted_code = \"# Initialize the model and optimizer\\nmodel = MultiHeadModel()\\nmodel = torch.compile(model)\\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\\n\\n# Initialize DeepSpeed\\nmodel, optimizer, _, _ = deepspeed.initialize(optimizer=optimizer,model=model,config='ds_config.json')\";\n                var nbb_formatted_code = \"# Initialize the model and optimizer\\nmodel = MultiHeadModel()\\nmodel = torch.compile(model)\\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\\n\\n# Initialize DeepSpeed\\nmodel, optimizer, _, _ = deepspeed.initialize(\\n    optimizer=optimizer, model=model, config=\\\"ds_config.json\\\"\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the model and optimizer\n",
    "model = MultiHeadModel()\n",
    "model = torch.compile(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Initialize DeepSpeed\n",
    "model, optimizer, _, _ = deepspeed.initialize(optimizer=optimizer,model=model,config='ds_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 7;\n                var nbb_unformatted_code = \"import random\\n\\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\\n\\nnum_samples = 10000\\nmax_length = 50\\n\\npositive_texts = [\\\"I absolutely love this product!\\\", \\n                  \\\"This is amazing, I'm so happy with it.\\\", \\n                  \\\"Fantastic experience, would recommend to anyone.\\\", \\n                  \\\"Great job, keep up the good work!\\\", \\n                  \\\"Excellent service, couldn't be happier.\\\"]\\n\\nnegative_texts = [\\\"I really hate this, it's awful.\\\", \\n                  \\\"This is terrible, would not recommend to anyone.\\\", \\n                  \\\"Awful experience, I'm so disappointed.\\\", \\n                  \\\"Bad job, this needs a lot of improvement.\\\", \\n                  \\\"Poor service, not happy at all.\\\"]\\n\\ntexts = []\\nsentiments = []\\n\\npersons = [\\\"John\\\", \\\"Emily\\\", \\\"Michael\\\", \\\"Sarah\\\"]\\norganizations = [\\\"Google\\\", \\\"Microsoft\\\", \\\"Apple\\\"]\\nlocations = [\\\"New York\\\", \\\"San Francisco\\\", \\\"London\\\"]\\nner_sentences = [\\n    \\\"[PERSON] works at [ORG].\\\",\\n    \\\"[PERSON] lives in [LOC].\\\",\\n    \\\"[ORG] is located in [LOC].\\\"\\n]\\nners = []\\n\\nfor _ in range(num_samples):\\n    # Sentiment\\n    if random.choice([True, False]):\\n        texts.append(random.choice(positive_texts))\\n        sentiments.append(1)\\n    else:\\n        texts.append(random.choice(negative_texts))\\n        sentiments.append(0)\\n\\n    # NER\\n    ner_sentence = random.choice(ner_sentences)\\n    ner_sentence = ner_sentence.replace(\\\"[PERSON]\\\", random.choice(persons))\\n    ner_sentence = ner_sentence.replace(\\\"[ORG]\\\", random.choice(organizations))\\n    ner_sentence = ner_sentence.replace(\\\"[LOC]\\\", random.choice(locations))\\n    ner_label_sequence = [0 if word not in persons + organizations + locations else persons.index(word) + 1 \\n                          if word in persons else organizations.index(word) + 5 \\n                          if word in organizations \\n                          else locations.index(word) + 8 \\n                          for word in ner_sentence.split()\\n                          ]\\n    \\n    ner_label_sequence += [0] * (max_length - len(ner_label_sequence))  # Padding\\n    ners.append(ner_label_sequence[:max_length])\\n\\nsentiments = torch.tensor(sentiments, dtype=torch.float32).view(-1, 1)\\nners = torch.tensor(ners, dtype=torch.long)\\n\\n# Tokenize the texts\\nencoding = tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\\ninput_ids = encoding['input_ids']\\nattention_mask = encoding['attention_mask']\\n\\ndataset = TensorDataset(input_ids, attention_mask, sentiments, ners)\\ntrain_size = int(0.8 * len(dataset))\\nval_size = len(dataset) - train_size\\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\\n\\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\";\n                var nbb_formatted_code = \"import random\\n\\ntokenizer = BertTokenizer.from_pretrained(\\\"bert-base-uncased\\\")\\n\\nnum_samples = 10000\\nmax_length = 50\\n\\npositive_texts = [\\n    \\\"I absolutely love this product!\\\",\\n    \\\"This is amazing, I'm so happy with it.\\\",\\n    \\\"Fantastic experience, would recommend to anyone.\\\",\\n    \\\"Great job, keep up the good work!\\\",\\n    \\\"Excellent service, couldn't be happier.\\\",\\n]\\n\\nnegative_texts = [\\n    \\\"I really hate this, it's awful.\\\",\\n    \\\"This is terrible, would not recommend to anyone.\\\",\\n    \\\"Awful experience, I'm so disappointed.\\\",\\n    \\\"Bad job, this needs a lot of improvement.\\\",\\n    \\\"Poor service, not happy at all.\\\",\\n]\\n\\ntexts = []\\nsentiments = []\\n\\npersons = [\\\"John\\\", \\\"Emily\\\", \\\"Michael\\\", \\\"Sarah\\\"]\\norganizations = [\\\"Google\\\", \\\"Microsoft\\\", \\\"Apple\\\"]\\nlocations = [\\\"New York\\\", \\\"San Francisco\\\", \\\"London\\\"]\\nner_sentences = [\\n    \\\"[PERSON] works at [ORG].\\\",\\n    \\\"[PERSON] lives in [LOC].\\\",\\n    \\\"[ORG] is located in [LOC].\\\",\\n]\\nners = []\\n\\nfor _ in range(num_samples):\\n    # Sentiment\\n    if random.choice([True, False]):\\n        texts.append(random.choice(positive_texts))\\n        sentiments.append(1)\\n    else:\\n        texts.append(random.choice(negative_texts))\\n        sentiments.append(0)\\n\\n    # NER\\n    ner_sentence = random.choice(ner_sentences)\\n    ner_sentence = ner_sentence.replace(\\\"[PERSON]\\\", random.choice(persons))\\n    ner_sentence = ner_sentence.replace(\\\"[ORG]\\\", random.choice(organizations))\\n    ner_sentence = ner_sentence.replace(\\\"[LOC]\\\", random.choice(locations))\\n    ner_label_sequence = [\\n        0\\n        if word not in persons + organizations + locations\\n        else persons.index(word) + 1\\n        if word in persons\\n        else organizations.index(word) + 5\\n        if word in organizations\\n        else locations.index(word) + 8\\n        for word in ner_sentence.split()\\n    ]\\n\\n    ner_label_sequence += [0] * (max_length - len(ner_label_sequence))  # Padding\\n    ners.append(ner_label_sequence[:max_length])\\n\\nsentiments = torch.tensor(sentiments, dtype=torch.float32).view(-1, 1)\\nners = torch.tensor(ners, dtype=torch.long)\\n\\n# Tokenize the texts\\nencoding = tokenizer(\\n    texts,\\n    padding=\\\"max_length\\\",\\n    truncation=True,\\n    max_length=max_length,\\n    return_tensors=\\\"pt\\\",\\n)\\ninput_ids = encoding[\\\"input_ids\\\"]\\nattention_mask = encoding[\\\"attention_mask\\\"]\\n\\ndataset = TensorDataset(input_ids, attention_mask, sentiments, ners)\\ntrain_size = int(0.8 * len(dataset))\\nval_size = len(dataset) - train_size\\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\\n\\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "num_samples = 10000\n",
    "max_length = 50\n",
    "\n",
    "positive_texts = [\"I absolutely love this product!\", \n",
    "                  \"This is amazing, I'm so happy with it.\", \n",
    "                  \"Fantastic experience, would recommend to anyone.\", \n",
    "                  \"Great job, keep up the good work!\", \n",
    "                  \"Excellent service, couldn't be happier.\"]\n",
    "\n",
    "negative_texts = [\"I really hate this, it's awful.\", \n",
    "                  \"This is terrible, would not recommend to anyone.\", \n",
    "                  \"Awful experience, I'm so disappointed.\", \n",
    "                  \"Bad job, this needs a lot of improvement.\", \n",
    "                  \"Poor service, not happy at all.\"]\n",
    "\n",
    "texts = []\n",
    "sentiments = []\n",
    "\n",
    "persons = [\"John\", \"Emily\", \"Michael\", \"Sarah\"]\n",
    "organizations = [\"Google\", \"Microsoft\", \"Apple\"]\n",
    "locations = [\"New York\", \"San Francisco\", \"London\"]\n",
    "ner_sentences = [\n",
    "    \"[PERSON] works at [ORG].\",\n",
    "    \"[PERSON] lives in [LOC].\",\n",
    "    \"[ORG] is located in [LOC].\"\n",
    "]\n",
    "ners = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    # Sentiment\n",
    "    if random.choice([True, False]):\n",
    "        texts.append(random.choice(positive_texts))\n",
    "        sentiments.append(1)\n",
    "    else:\n",
    "        texts.append(random.choice(negative_texts))\n",
    "        sentiments.append(0)\n",
    "\n",
    "    # NER\n",
    "    ner_sentence = random.choice(ner_sentences)\n",
    "    ner_sentence = ner_sentence.replace(\"[PERSON]\", random.choice(persons))\n",
    "    ner_sentence = ner_sentence.replace(\"[ORG]\", random.choice(organizations))\n",
    "    ner_sentence = ner_sentence.replace(\"[LOC]\", random.choice(locations))\n",
    "    ner_label_sequence = [0 if word not in persons + organizations + locations else persons.index(word) + 1 \n",
    "                          if word in persons else organizations.index(word) + 5 \n",
    "                          if word in organizations \n",
    "                          else locations.index(word) + 8 \n",
    "                          for word in ner_sentence.split()\n",
    "                          ]\n",
    "    \n",
    "    ner_label_sequence += [0] * (max_length - len(ner_label_sequence))  # Padding\n",
    "    ners.append(ner_label_sequence[:max_length])\n",
    "\n",
    "sentiments = torch.tensor(sentiments, dtype=torch.float32).view(-1, 1)\n",
    "ners = torch.tensor(ners, dtype=torch.long)\n",
    "\n",
    "# Tokenize the texts\n",
    "encoding = tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask, sentiments, ners)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.038848876953125\n",
      "Epoch 2, Loss: 0.039093017578125\n",
      "[2023-09-23 15:57:28,565] [INFO] [unfused_optimizer.py:289:_update_scale] No Grad overflow for 1000 iterations\n",
      "[2023-09-23 15:57:28,566] [INFO] [unfused_optimizer.py:290:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0\n",
      "Epoch 3, Loss: 0.040252685546875\n",
      "Epoch 4, Loss: 0.03826904296875\n",
      "Epoch 5, Loss: 0.04071044921875\n",
      "Epoch 6, Loss: 0.039306640625\n",
      "[2023-09-23 15:59:53,868] [INFO] [unfused_optimizer.py:289:_update_scale] No Grad overflow for 1000 iterations\n",
      "[2023-09-23 15:59:53,869] [INFO] [unfused_optimizer.py:290:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0\n",
      "Epoch 7, Loss: 0.038299560546875\n",
      "Epoch 8, Loss: 0.03857421875\n",
      "Epoch 9, Loss: 0.039398193359375\n",
      "Epoch 10, Loss: 0.038543701171875\n",
      "CPU times: user 6min 7s, sys: 300 ms, total: 6min 7s\n",
      "Wall time: 6min 7s\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 11;\n                var nbb_unformatted_code = \"%%time\\nfor epoch in range(10):\\n    for batch in train_loader:\\n        input_ids, attention_mask, sentiment_labels, ner_labels = batch\\n\\n        input_ids = input_ids.to(model.device)\\n        attention_mask = attention_mask.to(model.device)\\n        sentiment_labels = sentiment_labels.to(model.device)\\n        ner_labels = ner_labels.to(model.device)\\n\\n        sentiment_output, ner_output = model(input_ids, attention_mask)\\n        \\n        sentiment_loss = F.binary_cross_entropy_with_logits(sentiment_output, sentiment_labels)\\n        ner_loss = F.cross_entropy(ner_output.view(-1, 10), ner_labels.view(-1))\\n        loss = sentiment_loss + ner_loss\\n\\n        model.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n    print(f\\\"Epoch {epoch+1}, Loss: {loss.item()}\\\")\";\n                var nbb_formatted_code = \"%%time\\nfor epoch in range(10):\\n    for batch in train_loader:\\n        input_ids, attention_mask, sentiment_labels, ner_labels = batch\\n\\n        input_ids = input_ids.to(model.device)\\n        attention_mask = attention_mask.to(model.device)\\n        sentiment_labels = sentiment_labels.to(model.device)\\n        ner_labels = ner_labels.to(model.device)\\n\\n        sentiment_output, ner_output = model(input_ids, attention_mask)\\n        \\n        sentiment_loss = F.binary_cross_entropy_with_logits(sentiment_output, sentiment_labels)\\n        ner_loss = F.cross_entropy(ner_output.view(-1, 10), ner_labels.view(-1))\\n        loss = sentiment_loss + ner_loss\\n\\n        model.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n    print(f\\\"Epoch {epoch+1}, Loss: {loss.item()}\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(10):\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, sentiment_labels, ner_labels = batch\n",
    "\n",
    "        input_ids = input_ids.to(model.device)\n",
    "        attention_mask = attention_mask.to(model.device)\n",
    "        sentiment_labels = sentiment_labels.to(model.device)\n",
    "        ner_labels = ner_labels.to(model.device)\n",
    "\n",
    "        sentiment_output, ner_output = model(input_ids, attention_mask)\n",
    "        \n",
    "        sentiment_loss = F.binary_cross_entropy_with_logits(sentiment_output, sentiment_labels)\n",
    "        ner_loss = F.cross_entropy(ner_output.view(-1, 10), ner_labels.view(-1))\n",
    "        loss = sentiment_loss + ner_loss\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 9;\n                var nbb_unformatted_code = \"test_texts = [\\\"I love this max!\\\", \\\"This is terrible anna!\\\"]\\nencoding = tokenizer(test_texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\\ninput_ids = encoding['input_ids'].to(model.device)\\nattention_mask = encoding['attention_mask'].to(model.device)\\n\\nwith torch.no_grad():\\n    sentiment_output, ner_output = model(input_ids, attention_mask)\\n    sentiment_output = torch.sigmoid(sentiment_output)\\n    ner_output = torch.argmax(ner_output, dim=-1)\";\n                var nbb_formatted_code = \"test_texts = [\\\"I love this max!\\\", \\\"This is terrible anna!\\\"]\\nencoding = tokenizer(\\n    test_texts,\\n    padding=\\\"max_length\\\",\\n    truncation=True,\\n    max_length=max_length,\\n    return_tensors=\\\"pt\\\",\\n)\\ninput_ids = encoding[\\\"input_ids\\\"].to(model.device)\\nattention_mask = encoding[\\\"attention_mask\\\"].to(model.device)\\n\\nwith torch.no_grad():\\n    sentiment_output, ner_output = model(input_ids, attention_mask)\\n    sentiment_output = torch.sigmoid(sentiment_output)\\n    ner_output = torch.argmax(ner_output, dim=-1)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_texts = [\"I love this max!\", \"This is terrible anna!\"]\n",
    "encoding = tokenizer(test_texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "input_ids = encoding['input_ids'].to(model.device)\n",
    "attention_mask = encoding['attention_mask'].to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sentiment_output, ner_output = model(input_ids, attention_mask)\n",
    "    sentiment_output = torch.sigmoid(sentiment_output)\n",
    "    ner_output = torch.argmax(ner_output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: I love this max!\n",
      "  Sentiment: Positive\n",
      "Sentence 2: This is terrible anna!\n",
      "  Sentiment: Negative\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 10;\n                var nbb_unformatted_code = \"# Interpret the sentiment output\\nsentiment_output_np = sentiment_output.cpu().numpy()\\nsentiment_labels = [\\\"Positive\\\" if score > 0.5 else \\\"Negative\\\" for score in sentiment_output_np]\\n\\n# Interpret the NER output\\nner_output_np = ner_output.cpu().numpy()\\nner_classes = ['O', 'PERSON', 'ORG', 'LOC', 'DATE', 'TIME', 'MONEY', 'PERCENT', 'FAC', 'GPE']\\nner_labels = [[ner_classes[label] for label in sequence] for sequence in ner_output_np]\\n\\nfor i, (sentiment, ner) in enumerate(zip(sentiment_labels, ner_labels)):\\n    print(f\\\"Sentence {i+1}: {test_texts[i]}\\\")\\n    print(f\\\"  Sentiment: {sentiment}\\\")\\n    #print(f\\\"  NER Labels: {ner}\\\")\";\n                var nbb_formatted_code = \"# Interpret the sentiment output\\nsentiment_output_np = sentiment_output.cpu().numpy()\\nsentiment_labels = [\\n    \\\"Positive\\\" if score > 0.5 else \\\"Negative\\\" for score in sentiment_output_np\\n]\\n\\n# Interpret the NER output\\nner_output_np = ner_output.cpu().numpy()\\nner_classes = [\\n    \\\"O\\\",\\n    \\\"PERSON\\\",\\n    \\\"ORG\\\",\\n    \\\"LOC\\\",\\n    \\\"DATE\\\",\\n    \\\"TIME\\\",\\n    \\\"MONEY\\\",\\n    \\\"PERCENT\\\",\\n    \\\"FAC\\\",\\n    \\\"GPE\\\",\\n]\\nner_labels = [[ner_classes[label] for label in sequence] for sequence in ner_output_np]\\n\\nfor i, (sentiment, ner) in enumerate(zip(sentiment_labels, ner_labels)):\\n    print(f\\\"Sentence {i+1}: {test_texts[i]}\\\")\\n    print(f\\\"  Sentiment: {sentiment}\\\")\\n    # print(f\\\"  NER Labels: {ner}\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interpret the sentiment output\n",
    "sentiment_output_np = sentiment_output.cpu().numpy()\n",
    "sentiment_labels = [\"Positive\" if score > 0.5 else \"Negative\" for score in sentiment_output_np]\n",
    "\n",
    "# Interpret the NER output\n",
    "ner_output_np = ner_output.cpu().numpy()\n",
    "ner_classes = ['O', 'PERSON', 'ORG', 'LOC', 'DATE', 'TIME', 'MONEY', 'PERCENT', 'FAC', 'GPE']\n",
    "ner_labels = [[ner_classes[label] for label in sequence] for sequence in ner_output_np]\n",
    "\n",
    "for i, (sentiment, ner) in enumerate(zip(sentiment_labels, ner_labels)):\n",
    "    print(f\"Sentence {i+1}: {test_texts[i]}\")\n",
    "    print(f\"  Sentiment: {sentiment}\")\n",
    "    #print(f\"  NER Labels: {ner}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple multi-headed model\n",
    "class MultiHeadedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadedModel, self).__init__()\n",
    "        self.shared_layer = nn.Linear(10, 20)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classification_head = nn.Linear(20, 3)  # 3 classes\n",
    "        \n",
    "        # Regression head\n",
    "        self.regression_head = nn.Linear(20, 1)  # 1 output for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared_layer(x)\n",
    "        \n",
    "        # Classification output\n",
    "        classification_output = self.classification_head(x)\n",
    "        \n",
    "        # Regression output\n",
    "        regression_output = self.regression_head(x)\n",
    "        \n",
    "        return classification_output, regression_output\n",
    "\n",
    "# Custom loss function\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.classification_loss = nn.CrossEntropyLoss()\n",
    "        self.regression_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, classification_output, regression_output, classification_target, regression_target):\n",
    "        loss1 = self.classification_loss(classification_output, classification_target)\n",
    "        loss2 = self.regression_loss(regression_output, regression_target)\n",
    "        \n",
    "        # Combine the two losses in some way\n",
    "        combined_loss = loss1 + loss2\n",
    "        return combined_loss\n",
    "\n",
    "# Initialize model and loss\n",
    "model = MultiHeadedModel()\n",
    "criterion = CustomLoss()\n",
    "\n",
    "# Dummy data\n",
    "x = torch.randn(5, 10)  # 5 samples, 10 features\n",
    "classification_target = torch.tensor([0, 1, 2, 0, 1])  # 5 samples, 3 classes\n",
    "regression_target = torch.randn(5, 1)  # 5 samples, 1 output\n",
    "\n",
    "# Forward pass\n",
    "classification_output, regression_output = model(x)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(classification_output, regression_output, classification_target, regression_target)\n",
    "\n",
    "# Backward pass and optimization\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Combined Loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
